{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89913f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML/NLP Libraries\n",
    "import google.generativeai as genai\n",
    "import torch\n",
    "import spacy\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, util, losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Scikit-learn & LightGBM\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import (classification_report, accuracy_score, f1_score,\n",
    "                           mean_absolute_error)\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5916bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 11:59:51,065 - INFO - GPU available: NVIDIA A100-SXM4-80GB MIG 1g.20gb\n",
      "2025-04-06 11:59:51,274 - INFO - spaCy GPU preference set.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Check for GPU and set device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    spacy.prefer_gpu() # Tell spaCy to use GPU if available\n",
    "    logger.info(\"spaCy GPU preference set.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    logger.info(\"GPU not available, using CPU.\")\n",
    "    spacy.require_cpu()\n",
    "    logger.info(\"spaCy CPU preference set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463ca0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 12:00:52,417 - INFO - GEMINI_API_KEY loaded.\n",
      "2025-04-06 12:00:52,418 - INFO - Gemini configured.\n"
     ]
    }
   ],
   "source": [
    "GEMINI_API_KEY = \"AIzaSyBvLjAbdWz6v6_ii98B_j98IffX4TGrexM\"\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    logger.warning(\"GEMINI_API_KEY not found in environment variables.\")\n",
    "else:\n",
    "    logger.info(\"GEMINI_API_KEY loaded.\")\n",
    "    try:\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        logger.info(\"Gemini configured.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to configure Gemini: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1c9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model IDs & Paths ---\n",
    "SUMMARIZER_MODEL_ID_FINETUNE = \"google/flan-t5-base\" # Base for fine-tuning\n",
    "ENTITY_EXTRACTOR_MODEL_SPACY = \"en_core_web_trf\" # spaCy model\n",
    "ENTITY_EXTRACTOR_MODEL_ID_FINETUNE = \"bert-base-cased\" # Base for fine-tuning NER\n",
    "SENTENCE_TRANSFORMER_MODEL_ID = \"paraphrase-MiniLM-L6-v2\" # Bi-encoder\n",
    "\n",
    "# --- Local Paths (within notebook environment) ---\n",
    "DATA_DIR = \"./data\" # Create this directory if it doesn't exist\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"models\")\n",
    "INDEX_DIR = os.path.join(DATA_DIR, \"index\")\n",
    "HISTORICAL_TICKETS_PATH = os.path.join(DATA_DIR, \"dummy_historical_tickets.csv\")\n",
    "RETRIEVER_INDEX_PATH = os.path.join(INDEX_DIR, \"ticket_index_gpu.index\") # Use different name for GPU index\n",
    "RETRIEVER_MAP_PATH = RETRIEVER_INDEX_PATH + \".map\"\n",
    "ROUTER_MODEL_PATH = os.path.join(MODEL_DIR, \"router_pipeline.joblib\")\n",
    "TTR_ESTIMATOR_MODEL_PATH = os.path.join(MODEL_DIR, \"ttr_pipeline.joblib\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "# --- Training Hyperparameters (Examples) ---\n",
    "SFT_EPOCHS = 2\n",
    "SFT_LR = 1e-4\n",
    "NER_EPOCHS = 3\n",
    "NER_LR = 3e-5\n",
    "RETRIEVER_BATCH_SIZE = 32\n",
    "RETRIEVER_EPOCHS = 1\n",
    "\n",
    "# --- Other Settings ---\n",
    "DEFAULT_QUEUES = [\"Sync-Team\", \"Payments\", \"Networking\", \"General\"]\n",
    "RETRIEVER_TOP_K = 3\n",
    "TARGET_ENTITY_LABELS = { # Adjust based on actual needs and model used\n",
    "    \"issue_category\": [\"PRODUCT\", \"ORG\", \"EVENT\", \"WORK_OF_ART\"],\n",
    "    \"device\": [\"PRODUCT\"],\n",
    "    \"error_code\": [\"CARDINAL\", \"MONEY\", \"NORP\", \"FAC\"], # Wider net, needs refinement/rules\n",
    "    \"escalation_target\": [\"ORG\", \"PERSON\"],\n",
    "    \"promised_follow_up_date\": [\"DATE\", \"TIME\"]\n",
    "}\n",
    "TTR_TARGET_VARIABLE = 'ttr_hours'\n",
    "TTR_CATEGORICAL_FEATURES = ['predicted_queue', 'priority', 'day_of_week']\n",
    "TTR_NUMERICAL_FEATURES = ['sentiment_score', 'solution_similarity_score', 'agent_load_at_open']\n",
    "TTR_BOOLEAN_FEATURES = ['business_hours_flag']\n",
    "TTR_ALL_FEATURES = TTR_CATEGORICAL_FEATURES + TTR_NUMERICAL_FEATURES + TTR_BOOLEAN_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6525d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_historical_data(file_path=HISTORICAL_TICKETS_PATH):\n",
    "    \"\"\"Loads historical ticket data, creating dummy data if not found.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Loaded historical data from {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"{file_path} not found. Creating dummy data.\")\n",
    "        dummy_data = {\n",
    "            'ticket_id': [1, 2, 3, 4, 5, 6, 7],\n",
    "            'dialog': [\"User: My screen is black. Agent: Did you try turning it off and on again? User: Yes. Agent: Okay, let's check the cable.\",\n",
    "                       \"User: Can't login. Agent: Reset password link sent. User: Got it, thanks!\",\n",
    "                       \"User: Payment failed. Agent: I see the error code X45. It's a bank issue. User: Ah okay.\",\n",
    "                       \"User: How do I sync my calendar? Agent: Go to Settings > Accounts > Sync Now. User: Worked!\",\n",
    "                       \"User: Wifi slow. Agent: Have you rebooted the router? User: Doing it now. Agent: Let me know. User: Better, thanks!\",\n",
    "                       \"User: The app crashes when I click save. Error 503 pops up. Agent: Thanks, is this on iOS or Android? User: iOS. Agent: Okay, reporting bug to Sync-Team.\",\n",
    "                       \"User: My bill seems wrong. Charge for service XYZ is too high. Agent: Let me check your account... yes, I see the discrepancy. I'll adjust it. Should reflect in 24h.\"],\n",
    "            'summary': [\"User reported black screen, agent suggested reboot & cable check.\",\n",
    "                        \"User couldn't login, agent sent password reset link.\",\n",
    "                        \"User payment failed due to bank error X45.\",\n",
    "                        \"Agent guided user on how to sync calendar.\",\n",
    "                        \"User reported slow wifi, resolved after router reboot.\",\n",
    "                        \"App crashes with error 503 on iOS, agent reported bug.\",\n",
    "                        \"User disputed bill charge, agent found error and adjusted.\"],\n",
    "            'resolution': [\"Check display cable connection.\",\n",
    "                           \"Sent password reset link.\",\n",
    "                           \"Advised user it was a bank-side issue.\",\n",
    "                           \"Guided user through Settings > Accounts > Sync Now.\",\n",
    "                           \"Advised user to reboot router.\",\n",
    "                           \"Reported bug (Error 503, iOS) to Sync-Team.\",\n",
    "                           \"Corrected billing error for service XYZ.\"],\n",
    "            'queue': [\"Networking\", \"General\", \"Payments\", \"Sync-Team\", \"Networking\", \"Sync-Team\", \"Payments\"],\n",
    "            'ttr_hours': [2.5, 0.2, 0.5, 0.1, 0.3, 48.0, 24.0], # Added longer TTRs\n",
    "            'status': [\"solved\", \"solved\", \"solved\", \"solved\", \"solved\", \"open\", \"solved\"]\n",
    "        }\n",
    "        df = pd.DataFrame(dummy_data)\n",
    "        # Add dummy features for TTR estimator\n",
    "        df['priority'] = random.choices(['Low', 'Medium', 'High'], k=len(df))\n",
    "        df['timestamp'] = pd.to_datetime([datetime.datetime.now() - datetime.timedelta(hours=random.randint(1,100)) for _ in range(len(df))])\n",
    "        df['day_of_week'] = df['timestamp'].dt.strftime('%a')\n",
    "        df['business_hours_flag'] = df['timestamp'].apply(lambda ts: 8 <= ts.hour < 18 and ts.weekday() < 5).astype(str)\n",
    "        df['sentiment_score'] = [round(random.uniform(-0.8, 0.8), 2) for _ in range(len(df))]\n",
    "        df['solution_similarity_score'] = [round(random.uniform(0.3, 0.95), 3) for _ in range(len(df))]\n",
    "        df['agent_load_at_open'] = [random.randint(1, 8) for _ in range(len(df))]\n",
    "\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logger.info(f\"Created and saved dummy data to {file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading/creating historical data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "def preprocess_text_for_embedding(text):\n",
    "    \"\"\"Simple text cleaning.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.lower().strip()\n",
    "\n",
    "def log_feedback_notebook(ticket_id, component, feedback_data):\n",
    "    \"\"\"Simulates logging feedback in the notebook.\"\"\"\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    log_entry = f\"FEEDBACK @ {timestamp} | Ticket: {ticket_id} | Component: {component} | Data: {feedback_data}\"\n",
    "    print(log_entry)\n",
    "    # In a real scenario, append to a file or DataFrame\n",
    "    # with open(os.path.join(DATA_DIR, \"feedback_log.csv\"), \"a\") as f:\n",
    "    #     # Write as CSV row\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fafbadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 12:02:03,780 - INFO - Gemini model 'gemini-1.5-flash' initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load Gemini Model (if API key is available)\n",
    "gemini_model = None\n",
    "if GEMINI_API_KEY:\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\") # Or \"gemini-pro\"\n",
    "        logger.info(\"Gemini model 'gemini-1.5-flash' initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Gemini model: {e}\")\n",
    "else:\n",
    "    logger.warning(\"Gemini API Key not set. Summarization will be unavailable.\")\n",
    "\n",
    "# --- Summarization Function ---\n",
    "def summarize_conversation_gemini(conversation: str) -> str:\n",
    "    \"\"\"Summarizes using Gemini API, includes summary and action extraction.\"\"\"\n",
    "    if not gemini_model:\n",
    "        return \"Error: Gemini model not available.\"\n",
    "    if not conversation: return \"No conversation provided.\"\n",
    "\n",
    "    prompt = f\"\"\"You are a customer support assistant. Analyze the following conversation.\n",
    "1. Provide a crisp, 2-3 sentence summary focusing on the issue, agent actions, and outcome.\n",
    "2. Provide a bulleted list of the main action items or steps taken by the agent or promised to the user.\n",
    "\n",
    "Conversation:\n",
    "{conversation}\n",
    "\n",
    "Summary:\n",
    "[Your 2-3 sentence summary here]\n",
    "\n",
    "Actions:\n",
    "* [Action 1]\n",
    "* [Action 2]\n",
    "...\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        summary_text = response.text.strip()\n",
    "        # Extract summary and actions (improved regex)\n",
    "        summary_match = re.search(r\"Summary:\\n(.*?)\\nActions:\", summary_text, re.DOTALL | re.IGNORECASE)\n",
    "        actions_match = re.search(r\"Actions:\\n(.*?)$\", summary_text, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "        summary = summary_match.group(1).strip() if summary_match else \"Summary not extracted.\"\n",
    "        actions = actions_match.group(1).strip() if actions_match else \"Actions not extracted.\"\n",
    "\n",
    "        # Post-process and format\n",
    "        formatted_output = post_process_summary(summary, actions)\n",
    "        logger.info(\"Summarization successful using Gemini.\")\n",
    "        return formatted_output\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during Gemini summarization: {e}\")\n",
    "        return f\"Error: Could not generate summary ({e})\"\n",
    "\n",
    "# --- Post-processing ---\n",
    "def post_process_summary(summary: str, actions: str) -> str:\n",
    "    \"\"\"Runs simple checks and formats the summary output.\"\"\"\n",
    "    has_action_verb = any(verb in summary.lower() for verb in ['sent', 'guided', 'checked', 'reset', 'advised', 'suggested', 'resolved', 'fixed', 'updated', 'reported', 'adjusted', 'escalated'])\n",
    "    has_resolution_hint = any(hint in summary.lower() for hint in ['resolved', 'worked', 'fixed', 'thanks', 'okay', 'got it', 'better', 'sent', 'success', 'adjusted', 'corrected'])\n",
    "\n",
    "    warning = \"\"\n",
    "    if not has_action_verb: warning += \"[Warning: Summary may lack clear action verb.] \"\n",
    "    if not has_resolution_hint: warning += \"[Warning: Summary may lack clear resolution hint.]\"\n",
    "\n",
    "    # Ensure actions look like a list\n",
    "    if not actions.startswith('*'):\n",
    "        actions = \"* \" + actions.replace('\\n', '\\n* ')\n",
    "\n",
    "    formatted_output = f\"Summary: {summary.strip()}\\nActions:\\n{actions.strip()}\"\n",
    "    if warning: formatted_output += f\"\\n{warning.strip()}\"\n",
    "    return formatted_output\n",
    "\n",
    "# --- Main Summarizer Function ---\n",
    "def get_summary(conversation: str) -> str:\n",
    "    # In this notebook, we primarily rely on Gemini\n",
    "    return summarize_conversation_gemini(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f23546bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 12:02:17,934 - INFO - Summarization successful using Gemini.\n",
      "Summary: The user reported their SmartWidget X1 was frequently disconnecting from their Wifi. The agent diagnosed a weak signal (low RSSI value) as the likely cause.  The user successfully improved the connection by moving the widget closer to the router, and will monitor its performance.\n",
      "Actions:\n",
      "* Checked the SmartWidget's RSSI value via the device's settings page.\n",
      "* Determined that a low RSSI value (-75 dBm) indicated a weak signal strength.\n",
      "* Suggested and guided the user to move the SmartWidget closer to the router to improve signal strength.\n",
      "* Confirmed improved signal strength (-58 dBm) after relocation.\n",
      "* Advised the user to monitor the connection and consider a Wifi extender if the problem persists in the original location.\n",
      "* Provided troubleshooting advice and support throughout the interaction.\n"
     ]
    }
   ],
   "source": [
    "test_convo_summary = \"\"\"\n",
    "Agent: Support, how can I help?\n",
    "User: Hi, the SmartWidget X1 keeps disconnecting from my Wifi network every few hours. I already rebooted it.\n",
    "Agent: Got it. Let's check the signal strength. Can you access the widget's settings page? Go to Network > Status. What does it say for RSSI?\n",
    "User: Okay, accessing now... It says RSSI -75 dBm.\n",
    "Agent: Ah, that's quite low. Ideally, it should be better than -65 dBm. Is the widget far from your router?\n",
    "User: It's in the next room, maybe 20 feet away through one wall.\n",
    "Agent: Okay, -75 is borderline. Could you try moving the widget closer to the router, just temporarily, to see if the connection stabilizes?\n",
    "User: Sure, I'll move it now... Okay, it's about 5 feet away. The RSSI now shows -58 dBm.\n",
    "Agent: Excellent, that's much better. Let's keep it there for a while and see if the disconnections stop. If it stays connected, the location was likely the issue. We might need to look into a Wifi extender if you need it in the original spot.\n",
    "User: Okay, makes sense. I'll monitor it. Thanks!\n",
    "Agent: You're welcome! Let us know if it drops again.\n",
    "\"\"\"\n",
    "summary_result = get_summary(test_convo_summary)\n",
    "print(summary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0ab6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 12:02:50,995 - INFO - Loaded spaCy NER model 'en_core_web_trf' onto cuda.\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy Model\n",
    "nlp_ner = None\n",
    "try:\n",
    "    nlp_ner = spacy.load(ENTITY_EXTRACTOR_MODEL_SPACY, disable=[\"parser\"]) # Keep tagger, NER\n",
    "    nlp_ner.add_pipe('sentencizer')\n",
    "    logger.info(f\"Loaded spaCy NER model '{ENTITY_EXTRACTOR_MODEL_SPACY}' onto {DEVICE}.\")\n",
    "except OSError:\n",
    "    logger.error(f\"Failed to load spaCy model '{ENTITY_EXTRACTOR_MODEL_SPACY}'. Download it first.\")\n",
    "    logger.info(\"Attempting to load 'en_core_web_sm' as fallback...\")\n",
    "    try:\n",
    "        nlp_ner = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
    "        nlp_ner.add_pipe('sentencizer')\n",
    "        logger.info(\"Loaded fallback spaCy model 'en_core_web_sm'.\")\n",
    "    except OSError:\n",
    "         logger.error(\"Fallback spaCy model also not found. NER disabled.\")\n",
    "\n",
    "# --- Entity Extraction Function ---\n",
    "def extract_entities(text: str) -> dict:\n",
    "    \"\"\"Extracts predefined entities using the loaded spaCy model.\"\"\"\n",
    "    if not nlp_ner:\n",
    "        return {\"error\": \"NER model not available\"}\n",
    "    if not text: return {}\n",
    "\n",
    "    doc = nlp_ner(text)\n",
    "    entities_found = {key: [] for key in TARGET_ENTITY_LABELS.keys()}\n",
    "\n",
    "    logger.debug(f\"NER: Processing text ({len(text)} chars), found {len(doc.ents)} potential entities.\")\n",
    "    for ent in doc.ents:\n",
    "        logger.debug(f\" - Entity: '{ent.text}' ({ent.label_})\")\n",
    "        for target_entity, spacy_labels in TARGET_ENTITY_LABELS.items():\n",
    "            if ent.label_ in spacy_labels:\n",
    "                # Basic filtering (can be improved with rules)\n",
    "                entity_text = ent.text.strip()\n",
    "                # Rule for error codes (example: alphanumeric, length > 2)\n",
    "                if target_entity == \"error_code\" and not re.match(r'^[A-Za-z0-9]{3,}$', entity_text):\n",
    "                     continue\n",
    "                # Avoid adding duplicates\n",
    "                if entity_text not in entities_found[target_entity]:\n",
    "                    entities_found[target_entity].append(entity_text)\n",
    "\n",
    "    # Clean up empty lists\n",
    "    final_entities = {k: v for k, v in entities_found.items() if v}\n",
    "    logger.info(f\"Extracted entities: {final_entities}\")\n",
    "    return final_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d000e18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 12:03:56,445 - INFO - Extracted entities: {'issue_category': ['Visa', 'Acme Inc', 'X45', 'Payments', 'SmartWidget X1'], 'device': ['X45', 'SmartWidget X1'], 'escalation_target': ['Visa', 'Acme Inc', 'Payments'], 'promised_follow_up_date': ['tomorrow', '2024-07-28']}\n",
      "{'issue_category': ['Visa', 'Acme Inc', 'X45', 'Payments', 'SmartWidget X1'], 'device': ['X45', 'SmartWidget X1'], 'escalation_target': ['Visa', 'Acme Inc', 'Payments'], 'promised_follow_up_date': ['tomorrow', '2024-07-28']}\n"
     ]
    }
   ],
   "source": [
    "test_dialog_ner = \"\"\"\n",
    "User: My payment failed with error X45 on the website using my Visa ending in 1234. This was for the Acme Inc subscription.\n",
    "Agent: Okay, I see error X45 indicates a bank verification issue. I must escalate this to the Payments dept. Can I get your account ID?\n",
    "User: It's account-9876. Please ask them to check by tomorrow EOD, that's 2024-07-28.\n",
    "Agent: Will do. Escalation filed to Payments. They should review by 2024-07-28. Also, my SmartWidget X1 keeps crashing.\n",
    "\"\"\"\n",
    "entities_result = extract_entities(test_dialog_ner)\n",
    "print(entities_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a91564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer Model (will use GPU if detected)\n",
    "try:\n",
    "    bi_encoder = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL_ID, device=DEVICE)\n",
    "    EMBEDDING_DIM = bi_encoder.get_sentence_embedding_dimension()\n",
    "    logger.info(f\"Loaded Sentence Transformer '{SENTENCE_TRANSFORMER_MODEL_ID}' onto {DEVICE}. Dim: {EMBEDDING_DIM}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Sentence Transformer model: {e}\")\n",
    "    bi_encoder = None\n",
    "    EMBEDDING_DIM = None\n",
    "\n",
    "# --- FAISS Indexing ---\n",
    "faiss_index = None\n",
    "index_to_ticket_data = {} # Map FAISS index ID -> {'resolution': ..., 'ticket_id': ...}\n",
    "\n",
    "def build_or_load_retriever_index(force_rebuild=False):\n",
    "    \"\"\"Builds or loads the FAISS index for historical tickets (GPU).\"\"\"\n",
    "    global faiss_index, index_to_ticket_data\n",
    "\n",
    "    if not force_rebuild and os.path.exists(RETRIEVER_INDEX_PATH) and os.path.exists(RETRIEVER_MAP_PATH):\n",
    "        try:\n",
    "            logger.info(f\"Loading existing FAISS index from {RETRIEVER_INDEX_PATH}\")\n",
    "            # Load CPU index first, then move to GPU\n",
    "            # cpu_index = faiss.read_index(RETRIEVER_INDEX_PATH)\n",
    "            # Load directly if saved from GPU resource? Test this.\n",
    "            faiss_index = faiss.read_index(RETRIEVER_INDEX_PATH)\n",
    "\n",
    "            if DEVICE.type == 'cuda':\n",
    "                 # If GPU available, try moving the loaded index to GPU\n",
    "                 # This requires the index to be compatible or requires cloning\n",
    "                 res = faiss.StandardGpuResources() # Get GPU resources\n",
    "                 co = faiss.GpuClonerOptions()\n",
    "                 # Clone to GPU (might need specific index types)\n",
    "                 try:\n",
    "                      faiss_index = faiss.index_cpu_to_gpu(res, DEVICE.index or 0, faiss_index, co)\n",
    "                      logger.info(\"Successfully moved loaded FAISS index to GPU.\")\n",
    "                 except Exception as clone_err:\n",
    "                      logger.warning(f\"Could not automatically move loaded index to GPU: {clone_err}. Using CPU index.\")\n",
    "                      # Fallback or keep the CPU index if cloning fails\n",
    "\n",
    "            with open(RETRIEVER_MAP_PATH, 'rb') as f:\n",
    "                index_to_ticket_data = pickle.load(f)\n",
    "            logger.info(f\"Loaded FAISS index with {faiss_index.ntotal} vectors and map.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading index: {e}. Rebuilding.\")\n",
    "\n",
    "    logger.info(\"Building new retriever index...\")\n",
    "    if not bi_encoder:\n",
    "        logger.error(\"Cannot build index: Sentence Transformer model not loaded.\")\n",
    "        return False\n",
    "\n",
    "    df_history = load_historical_data()\n",
    "    if df_history.empty: logger.error(\"No historical data.\"); return False\n",
    "\n",
    "    # Index based on dialog for better context\n",
    "    df_solved = df_history[df_history['status'].str.lower() == 'solved'].copy()\n",
    "    if df_solved.empty: logger.error(\"No 'solved' tickets found.\"); return False\n",
    "    logger.info(f\"Indexing {len(df_solved)} solved tickets...\")\n",
    "\n",
    "    texts_to_embed = [preprocess_text_for_embedding(text) for text in df_solved['dialog'].fillna('')]\n",
    "    ticket_ids = df_solved['ticket_id'].tolist()\n",
    "    resolutions = df_solved['resolution'].fillna('N/A').tolist()\n",
    "\n",
    "    logger.info(\"Encoding corpus (using GPU if available)...\")\n",
    "    corpus_embeddings = bi_encoder.encode(texts_to_embed, convert_to_tensor=True, show_progress_bar=True, device=DEVICE)\n",
    "    corpus_embeddings_np = corpus_embeddings.cpu().numpy() # Move to CPU for FAISS build/normalize\n",
    "    faiss.normalize_L2(corpus_embeddings_np) # Normalize for cosine similarity\n",
    "\n",
    "    logger.info(\"Building FAISS index...\")\n",
    "    # Use IndexFlatL2 for CPU, potentially different for GPU optimization later\n",
    "    cpu_index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        try:\n",
    "            res = faiss.StandardGpuResources() # Use default GPU resources\n",
    "            gpu_index = faiss.index_cpu_to_gpu(res, DEVICE.index or 0, cpu_index)\n",
    "            logger.info(\"Created FAISS index on GPU.\")\n",
    "            faiss_index = gpu_index\n",
    "        except Exception as gpu_err:\n",
    "            logger.warning(f\"Failed to create FAISS index on GPU: {gpu_err}. Using CPU index.\")\n",
    "            faiss_index = cpu_index # Fallback to CPU index\n",
    "    else:\n",
    "        faiss_index = cpu_index # Use CPU index if no GPU\n",
    "\n",
    "    faiss_index.add(corpus_embeddings_np) # Add normalized embeddings\n",
    "\n",
    "    # Create mapping from FAISS index position (0 to n-1) to ticket info\n",
    "    index_to_ticket_data = {i: {'resolution': res, 'ticket_id': tid}\n",
    "                           for i, (tid, res) in enumerate(zip(ticket_ids, resolutions))}\n",
    "\n",
    "    logger.info(f\"Index build complete. Indexed {faiss_index.ntotal} documents.\")\n",
    "\n",
    "    # Save the index (move back to CPU first if it's on GPU) and map\n",
    "    try:\n",
    "        index_to_save = faiss_index\n",
    "        if DEVICE.type == 'cuda' and isinstance(faiss_index, faiss.GpuIndex):\n",
    "             logger.info(\"Moving FAISS index to CPU for saving.\")\n",
    "             index_to_save = faiss.index_gpu_to_cpu(faiss_index)\n",
    "\n",
    "        faiss.write_index(index_to_save, RETRIEVER_INDEX_PATH)\n",
    "        with open(RETRIEVER_MAP_PATH, 'wb') as f:\n",
    "            pickle.dump(index_to_ticket_data, f)\n",
    "        logger.info(f\"Saved FAISS index to {RETRIEVER_INDEX_PATH} and map to {RETRIEVER_MAP_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving index: {e}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# --- Retrieval Function ---\n",
    "def retrieve_solutions(query_text: str, top_k: int = RETRIEVER_TOP_K) -> list[dict]:\n",
    "    \"\"\"Retrieves top_k relevant resolutions using FAISS and bi-encoder.\"\"\"\n",
    "    if not faiss_index or not index_to_ticket_data:\n",
    "        logger.error(\"Retriever index not ready.\")\n",
    "        return [{\"error\": \"Retriever not initialized\"}]\n",
    "    if not query_text: return []\n",
    "\n",
    "    logger.info(f\"Retrieving solutions for query: '{query_text[:100]}...'\")\n",
    "    query_embedding = bi_encoder.encode(preprocess_text_for_embedding(query_text),\n",
    "                                        convert_to_tensor=True, device=DEVICE)\n",
    "    query_embedding_np = query_embedding.cpu().numpy().reshape(1, -1)\n",
    "    faiss.normalize_L2(query_embedding_np) # Normalize query\n",
    "\n",
    "    # Search the index\n",
    "    distances, indices = faiss_index.search(query_embedding_np, top_k)\n",
    "\n",
    "    results = []\n",
    "    if indices.size > 0:\n",
    "        for i in range(indices.shape[1]):\n",
    "            idx = indices[0, i]\n",
    "            if idx != -1 and idx in index_to_ticket_data:\n",
    "                score = distances[0, i] # For L2 index, lower distance is better. Convert to similarity?\n",
    "                # Convert L2 distance to cosine similarity: sim = 1 - (dist^2 / 2) for normalized vectors\n",
    "                similarity = 1 - (score**2 / 2)\n",
    "                ticket_info = index_to_ticket_data[idx]\n",
    "                results.append({\n",
    "                    \"ticket_id\": ticket_info['ticket_id'],\n",
    "                    \"resolution\": ticket_info['resolution'],\n",
    "                    \"score\": float(similarity) # Return cosine similarity\n",
    "                })\n",
    "            else:\n",
    "                 logger.warning(f\"Retrieved invalid index {idx} or not found in map.\")\n",
    "\n",
    "    # Sort by similarity score (higher is better)\n",
    "    results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
    "    logger.info(f\"Retrieved {len(results)} solutions.\")\n",
    "    return results[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index (can take time depending on data size and hardware)\n",
    "# Set force_rebuild=True if you updated the data or model\n",
    "index_ready = build_or_load_retriever_index(force_rebuild=False)\n",
    "if not index_ready:\n",
    "    logger.error(\"Failed to build or load the retriever index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_ready:\n",
    "    test_query_retrieval = \"user wifi keeps dropping very slow\"\n",
    "    recommendations = retrieve_solutions(test_query_retrieval)\n",
    "    print(\"\\n--- Recommended Resolutions ---\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"- Ticket ID: {rec['ticket_id']}, Score: {rec['score']:.4f}, Resolution: {rec['resolution']}\")\n",
    "else:\n",
    "    print(\"\\nRetriever index not available. Skipping retrieval example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_pipeline = None\n",
    "\n",
    "# --- Router Training Function ---\n",
    "def train_router():\n",
    "    global router_pipeline\n",
    "    logger.info(\"Starting task router training...\")\n",
    "    df_history = load_historical_data()\n",
    "    target_col = 'queue'\n",
    "    text_feature = 'summary' # Use summary for routing\n",
    "\n",
    "    if df_history.empty or target_col not in df_history.columns or text_feature not in df_history.columns:\n",
    "        logger.error(\"Insufficient data for training router.\"); return False\n",
    "\n",
    "    df_train = df_history.dropna(subset=[text_feature, target_col]).copy()\n",
    "    if len(df_train) < 10: logger.error(f\"Need more data ({len(df_train)} rows) for router.\"); return False\n",
    "\n",
    "    X = df_train[text_feature]\n",
    "    y = df_train[target_col]\n",
    "    known_classes = sorted(y.unique())\n",
    "    logger.info(f\"Router classes: {known_classes}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1,2))),\n",
    "        ('clf', LogisticRegression(random_state=42, class_weight='balanced', C=1.0, max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    logger.info(\"Training the router pipeline...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    logger.info(f\"Router Eval - Accuracy: {accuracy:.4f}, Macro F1: {macro_f1:.4f}\")\n",
    "    # print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    try:\n",
    "        joblib.dump(pipeline, ROUTER_MODEL_PATH)\n",
    "        logger.info(f\"Saved trained router model to {ROUTER_MODEL_PATH}\")\n",
    "        router_pipeline = pipeline # Update global var\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving router model: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Router Loading Function ---\n",
    "def load_router_model():\n",
    "    global router_pipeline\n",
    "    if os.path.exists(ROUTER_MODEL_PATH):\n",
    "        try:\n",
    "            router_pipeline = joblib.load(ROUTER_MODEL_PATH)\n",
    "            logger.info(f\"Loaded task router model from {ROUTER_MODEL_PATH}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading router model: {e}\")\n",
    "            router_pipeline = None\n",
    "    return False\n",
    "\n",
    "# --- Router Prediction Function ---\n",
    "def predict_queue(ticket_summary: str) -> str:\n",
    "    if not router_pipeline:\n",
    "        logger.warning(\"Router model not loaded. Returning default queue.\")\n",
    "        return DEFAULT_QUEUES[-1] # General queue\n",
    "\n",
    "    try:\n",
    "        # Input must be iterable\n",
    "        prediction = router_pipeline.predict([ticket_summary])\n",
    "        predicted = prediction[0]\n",
    "        # Ensure prediction is within known classes from training (if possible)\n",
    "        # classes = router_pipeline.classes_ # Access classes if needed\n",
    "        logger.info(f\"Predicted queue: {predicted}\")\n",
    "        return predicted\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during queue prediction: {e}\")\n",
    "        return DEFAULT_QUEUES[-1] # Fallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d25311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not load_router_model():\n",
    "    logger.info(\"Router model not found, attempting to train...\")\n",
    "    train_router()\n",
    "else:\n",
    "    logger.info(\"Router model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51143287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if router_pipeline:\n",
    "    test_summary_route = \"App crashes with error 503 on iOS during sync\"\n",
    "    predicted_q = predict_queue(test_summary_route)\n",
    "    print(f\"Summary: '{test_summary_route}' -> Predicted Queue: {predicted_q}\")\n",
    "\n",
    "    test_summary_route_2 = \"cannot make payment with visa card error x45\"\n",
    "    predicted_q_2 = predict_queue(test_summary_route_2)\n",
    "    print(f\"Summary: '{test_summary_route_2}' -> Predicted Queue: {predicted_q_2}\")\n",
    "else:\n",
    "    print(\"Router model not available for prediction example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ttr_pipeline = None\n",
    "ttr_feature_names_out = None\n",
    "\n",
    "# Transformer for log/exp target transformation\n",
    "log_transformer = FunctionTransformer(np.log1p, np.expm1, validate=True)\n",
    "\n",
    "# --- TTR Training Function ---\n",
    "def train_ttr_estimator():\n",
    "    global ttr_pipeline, ttr_feature_names_out\n",
    "    logger.info(\"Starting TTR estimator training...\")\n",
    "    df_history = load_historical_data()\n",
    "\n",
    "    if df_history.empty or TTR_TARGET_VARIABLE not in df_history.columns:\n",
    "        logger.error(f\"TTR training failed: Need '{TTR_TARGET_VARIABLE}' column.\"); return False\n",
    "\n",
    "    # Ensure all required feature columns exist (might need dummy data for initial run)\n",
    "    missing_cols = [col for col in TTR_ALL_FEATURES if col not in df_history.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"TTR training failed: Missing required feature columns: {missing_cols}\")\n",
    "        logger.error(\"Ensure load_historical_data provides these or add them.\")\n",
    "        return False # Stop if features are missing\n",
    "\n",
    "    df_train = df_history.dropna(subset=[TTR_TARGET_VARIABLE] + TTR_ALL_FEATURES).copy()\n",
    "\n",
    "    # Convert boolean flags to string for categorical encoding\n",
    "    for col in TTR_BOOLEAN_FEATURES:\n",
    "        df_train[col] = df_train[col].astype(str)\n",
    "\n",
    "    if len(df_train) < 10: logger.error(f\"Need more data ({len(df_train)} rows) for TTR.\"); return False\n",
    "\n",
    "    X = df_train[TTR_ALL_FEATURES]\n",
    "    y = df_train[TTR_TARGET_VARIABLE]\n",
    "    y_log = log_transformer.transform(y) # Log-transform target\n",
    "\n",
    "    X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), TTR_CATEGORICAL_FEATURES + TTR_BOOLEAN_FEATURES),\n",
    "            ('num', StandardScaler(), TTR_NUMERICAL_FEATURES)\n",
    "        ],\n",
    "        remainder='passthrough' # Should be empty if X includes only specified features\n",
    "    )\n",
    "\n",
    "    # Define Model (LightGBM)\n",
    "    # Note: For GPU with LightGBM, need 'device':'gpu' and potentially build from source with GPU support\n",
    "    lgbm = lgb.LGBMRegressor(random_state=42, n_estimators=100, learning_rate=0.1, num_leaves=31)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('regressor', lgbm)\n",
    "    ])\n",
    "\n",
    "    logger.info(\"Training the TTR pipeline...\")\n",
    "    pipeline.fit(X_train, y_train_log)\n",
    "\n",
    "    # Get feature names after preprocessing (important for prediction)\n",
    "    try:\n",
    "        ttr_feature_names_out = pipeline.named_steps['preprocess'].get_feature_names_out()\n",
    "        logger.info(f\"TTR Feature names out: {ttr_feature_names_out.tolist()}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not get feature names from TTR preprocessor: {e}\")\n",
    "        ttr_feature_names_out = X_train.columns.tolist() # Fallback\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred_log = pipeline.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log) # Inverse transform predictions\n",
    "    y_test = np.expm1(y_test_log) # Inverse transform true values\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    logger.info(f\"TTR Eval - MAE: {mae:.4f} hours\")\n",
    "\n",
    "    try:\n",
    "        # Save pipeline AND feature names\n",
    "        joblib.dump({'pipeline': pipeline, 'feature_names': ttr_feature_names_out}, TTR_ESTIMATOR_MODEL_PATH)\n",
    "        logger.info(f\"Saved trained TTR estimator model to {TTR_ESTIMATOR_MODEL_PATH}\")\n",
    "        ttr_pipeline = pipeline # Update global var\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving TTR estimator model: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- TTR Loading Function ---\n",
    "def load_ttr_model():\n",
    "    global ttr_pipeline, ttr_feature_names_out\n",
    "    if os.path.exists(TTR_ESTIMATOR_MODEL_PATH):\n",
    "        try:\n",
    "            data = joblib.load(TTR_ESTIMATOR_MODEL_PATH)\n",
    "            ttr_pipeline = data['pipeline']\n",
    "            ttr_feature_names_out = data['feature_names']\n",
    "            logger.info(f\"Loaded TTR estimator model from {TTR_ESTIMATOR_MODEL_PATH}\")\n",
    "            if ttr_feature_names_out:\n",
    "                 logger.info(f\"Loaded TTR feature names: {ttr_feature_names_out.tolist()}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading TTR model: {e}\")\n",
    "            ttr_pipeline = None\n",
    "            ttr_feature_names_out = None\n",
    "    return False\n",
    "\n",
    "# --- TTR Prediction Function ---\n",
    "def predict_ttr(features: dict) -> float:\n",
    "    if not ttr_pipeline or not ttr_feature_names_out:\n",
    "        logger.warning(\"TTR estimator model or feature names not loaded. Returning -1.\")\n",
    "        return -1.0\n",
    "\n",
    "    try:\n",
    "        # Create DataFrame with columns in the exact order expected by the pipeline\n",
    "        input_df = pd.DataFrame([features])\n",
    "         # Ensure all expected columns are present, add missing with NaN/default\n",
    "        for col in ttr_feature_names_out:\n",
    "            # Extract base feature name if transformers added prefixes (e.g., 'cat__')\n",
    "            base_col = col.split('__')[-1]\n",
    "            if base_col not in input_df.columns:\n",
    "                 logger.warning(f\"Missing feature '{base_col}' in input dict for TTR. Adding default.\")\n",
    "                 # Add sensible defaults (needs improvement based on actual feature types)\n",
    "                 input_df[base_col] = 0.0 if base_col in TTR_NUMERICAL_FEATURES else 'Unknown'\n",
    "\n",
    "        # Reorder input_df columns to match feature_names_out exactly\n",
    "        # This requires mapping the original feature names to the potentially prefixed names\n",
    "        input_reordered_df = pd.DataFrame(columns=ttr_feature_names_out)\n",
    "        # This mapping part is tricky, assuming ColumnTransformer output format\n",
    "        col_map = {}\n",
    "        cat_bool_cols = TTR_CATEGORICAL_FEATURES + TTR_BOOLEAN_FEATURES\n",
    "        num_cols = TTR_NUMERICAL_FEATURES\n",
    "        current_cat_idx = 0\n",
    "        current_num_idx = 0\n",
    "        for proc_col_name in ttr_feature_names_out:\n",
    "             if proc_col_name.startswith('cat__'):\n",
    "                  orig_col = cat_bool_cols[current_cat_idx]\n",
    "                  input_reordered_df[proc_col_name] = input_df[orig_col]\n",
    "                  current_cat_idx +=1\n",
    "             elif proc_col_name.startswith('num__'):\n",
    "                  orig_col = num_cols[current_num_idx]\n",
    "                  input_reordered_df[proc_col_name] = input_df[orig_col]\n",
    "                  current_num_idx += 1\n",
    "             else: # Passthrough? Should not happen if remainder='drop' or defined fully\n",
    "                  input_reordered_df[proc_col_name] = input_df[proc_col_name] # Assume name matches\n",
    "\n",
    "\n",
    "        # Predict log-transformed TTR\n",
    "        log_ttr_pred = ttr_pipeline.predict(input_reordered_df)\n",
    "        # Inverse transform to get actual TTR\n",
    "        predicted_ttr = np.expm1(log_ttr_pred)\n",
    "        predicted_ttr = max(0.0, predicted_ttr[0]) # Ensure non-negative\n",
    "\n",
    "        logger.info(f\"Predicted TTR (hours): {predicted_ttr:.2f}\")\n",
    "        return predicted_ttr\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during TTR prediction: {e}\", exc_info=True)\n",
    "        return -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8770e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not load_ttr_model():\n",
    "    logger.info(\"TTR estimator model not found, attempting to train...\")\n",
    "    train_ttr_estimator()\n",
    "else:\n",
    "    logger.info(\"TTR estimator model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b18ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ttr_pipeline:\n",
    "    # Example features (match what the model expects)\n",
    "    test_features_ttr = {\n",
    "        'predicted_queue': 'Networking', # From router output\n",
    "        'priority': 'High',             # From ticket metadata\n",
    "        'day_of_week': 'Tue',           # Calculated from timestamp\n",
    "        'sentiment_score': -0.5,        # From sentiment analysis (dummy here)\n",
    "        'solution_similarity_score': 0.85, # From retriever output (dummy here)\n",
    "        'agent_load_at_open': 6.0,      # From system data (dummy here)\n",
    "        'business_hours_flag': 'True'   # Calculated from timestamp\n",
    "    }\n",
    "    predicted_ttr_result = predict_ttr(test_features_ttr)\n",
    "    print(f\"\\nInput Features: {test_features_ttr}\")\n",
    "    print(f\"Predicted TTR: {predicted_ttr_result:.2f} hours\")\n",
    "else:\n",
    "    print(\"\\nTTR Estimator model not available for prediction example.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticket_notebook(ticket_id: str, conversation: str, priority: str = \"Medium\", timestamp: datetime.datetime = None):\n",
    "    \"\"\"Runs the full pipeline on a given conversation.\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"--- Processing Ticket {ticket_id} ---\")\n",
    "    if timestamp is None: timestamp = datetime.datetime.now()\n",
    "    warnings = []\n",
    "    results = {\"ticket_id\": ticket_id}\n",
    "\n",
    "    # 1. Summarization\n",
    "    summary_output = get_summary(conversation)\n",
    "    if \"Error:\" in summary_output:\n",
    "        warnings.append(\"Summarization failed.\")\n",
    "        results['summary'] = summary_output\n",
    "        summary_for_downstream = \"\" # Use empty summary if failed\n",
    "    else:\n",
    "        results['summary'] = summary_output\n",
    "        # Extract just the summary part for downstream tasks if needed\n",
    "        summary_match = re.search(r\"Summary:\\n(.*?)\\nActions:\", summary_output, re.DOTALL | re.IGNORECASE)\n",
    "        summary_for_downstream = summary_match.group(1).strip() if summary_match else summary_output # Fallback\n",
    "\n",
    "    # 2. Entity Extraction\n",
    "    entities = extract_entities(conversation) # Use full convo for NER\n",
    "    if \"error\" in entities:\n",
    "        warnings.append(\"Entity extraction failed.\")\n",
    "        results['entities'] = {}\n",
    "    else:\n",
    "        results['entities'] = entities\n",
    "\n",
    "    # 3. Task Routing\n",
    "    if router_pipeline:\n",
    "        predicted_queue = predict_queue(summary_for_downstream) # Use extracted summary\n",
    "        results['predicted_queue'] = predicted_queue\n",
    "    else:\n",
    "        warnings.append(\"Router model not loaded.\")\n",
    "        results['predicted_queue'] = DEFAULT_QUEUES[-1] # Default\n",
    "\n",
    "    # 4. Resolution Recommendation\n",
    "    if index_ready:\n",
    "        recommended_solutions = retrieve_solutions(summary_for_downstream) # Use extracted summary\n",
    "        if any(\"error\" in sol for sol in recommended_solutions):\n",
    "             warnings.append(\"Resolution retrieval failed.\")\n",
    "             results['recommended_solutions'] = []\n",
    "        else:\n",
    "             results['recommended_solutions'] = recommended_solutions\n",
    "    else:\n",
    "        warnings.append(\"Retriever index not ready.\")\n",
    "        results['recommended_solutions'] = []\n",
    "\n",
    "    # 5. TTR Estimation\n",
    "    if ttr_pipeline:\n",
    "        # Construct features - NEEDS REAL DATA MAPPING\n",
    "        sentiment_score = round(random.uniform(-1, 1), 2) # Placeholder\n",
    "        solution_similarity_score = results['recommended_solutions'][0]['score'] if results['recommended_solutions'] else 0.0\n",
    "        agent_load_at_open = random.randint(1, 10) # Placeholder\n",
    "        business_hours_flag = 8 <= timestamp.hour < 18 and timestamp.weekday() < 5\n",
    "        day_of_week = timestamp.strftime('%a')\n",
    "\n",
    "        ttr_features = {\n",
    "            'predicted_queue': results['predicted_queue'],\n",
    "            'priority': priority,\n",
    "            'sentiment_score': sentiment_score,\n",
    "            'solution_similarity_score': solution_similarity_score,\n",
    "            'business_hours_flag': str(business_hours_flag),\n",
    "            'day_of_week': day_of_week,\n",
    "            'agent_load_at_open': float(agent_load_at_open)\n",
    "            # Add other features...\n",
    "        }\n",
    "        predicted_ttr = predict_ttr(ttr_features)\n",
    "        if predicted_ttr < 0:\n",
    "            warnings.append(\"TTR prediction failed.\")\n",
    "            results['predicted_ttr_hours'] = -1.0\n",
    "        else:\n",
    "            results['predicted_ttr_hours'] = predicted_ttr\n",
    "    else:\n",
    "        warnings.append(\"TTR Estimator model not loaded.\")\n",
    "        results['predicted_ttr_hours'] = -1.0\n",
    "\n",
    "    end_time = time.time()\n",
    "    processing_time_ms = (end_time - start_time) * 1000\n",
    "    results['processing_time_ms'] = processing_time_ms\n",
    "    results['warnings'] = warnings\n",
    "\n",
    "    logger.info(f\"--- Finished Processing Ticket {ticket_id} in {processing_time_ms:.2f} ms ---\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_ticket_id = \"NB-9876\"\n",
    "sample_conversation = \"\"\"\n",
    "User: Hey, my internet seems really slow today, especially when streaming video. It keeps buffering.\n",
    "Agent: Hi there, let's check that out. Have you already tried restarting your modem and router?\n",
    "User: Yes, I did that first thing. Didn't seem to change anything. My plan is supposed to be 500 Mbps.\n",
    "Agent: Okay, thanks. Can you run a speed test at speedtest.net and tell me the download and upload speeds?\n",
    "User: Sure, running it now... Hmm, getting about 50 Mbps down and 15 Mbps up. Way lower than usual.\n",
    "Agent: Right, that's definitely low for your plan. Let me check the line from our end... I'm seeing some signal noise upstream. Could you check the coaxial cable connection at the back of the modem and at the wall outlet? Make sure they are finger-tight.\n",
    "User: Let me look... The one at the modem was a little loose. I tightened it up.\n",
    "Agent: Great. Could you please reboot the modem one more time now that the cable is secure? Just unplug the power for 30 seconds.\n",
    "User: Okay, rebooting... [waits a minute] ...It's back online.\n",
    "Agent: Perfect. Can you run that speed test again, please?\n",
    "User: Running now... Wow, much better! 480 Mbps down, 22 Mbps up. Looks like that loose cable was the issue!\n",
    "Agent: Excellent! Glad we could sort that out. That noise I saw on the line is gone now too. Anything else I can help with?\n",
    "User: Nope, that fixed it. Thanks so much!\n",
    "Agent: You're welcome! Have a good day.\n",
    "\"\"\"\n",
    "\n",
    "pipeline_results = process_ticket_notebook(sample_ticket_id, sample_conversation, priority=\"Medium\")\n",
    "\n",
    "# Print the results nicely\n",
    "print(\"\\n--- Pipeline Results ---\")\n",
    "print(f\"Ticket ID: {pipeline_results.get('ticket_id')}\")\n",
    "print(f\"\\nSummary & Actions:\\n{pipeline_results.get('summary', 'N/A')}\")\n",
    "print(f\"\\nEntities: {pipeline_results.get('entities', {})}\")\n",
    "print(f\"\\nPredicted Queue: {pipeline_results.get('predicted_queue', 'N/A')}\")\n",
    "print(f\"\\nPredicted TTR (hours): {pipeline_results.get('predicted_ttr_hours', 'N/A'):.2f}\")\n",
    "print(\"\\nRecommended Solutions:\")\n",
    "for sol in pipeline_results.get('recommended_solutions', []):\n",
    "    print(f\"  - Ticket ID: {sol['ticket_id']}, Score: {sol['score']:.4f}, Resolution: {sol['resolution']}\")\n",
    "if not pipeline_results.get('recommended_solutions'): print(\"  (None)\")\n",
    "print(f\"\\nProcessing Time (ms): {pipeline_results.get('processing_time_ms', 0):.2f}\")\n",
    "if pipeline_results.get('warnings'):\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in pipeline_results['warnings']: print(f\"  - {w}\")\n",
    "print(\"--- End Results ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate agent feedback on the retrieved solutions\n",
    "if pipeline_results.get('recommended_solutions'):\n",
    "    # Agent found the first solution helpful\n",
    "    feedback_data_helpful = {\n",
    "        \"feedback_type\": \"solution_helpful\",\n",
    "        \"solution_ticket_id\": pipeline_results['recommended_solutions'][0]['ticket_id'],\n",
    "        \"agent_id\": \"agent007\"\n",
    "    }\n",
    "    log_feedback_notebook(sample_ticket_id, \"retriever\", feedback_data_helpful)\n",
    "\n",
    "    # Agent found the second solution not helpful\n",
    "    if len(pipeline_results['recommended_solutions']) > 1:\n",
    "        feedback_data_not_helpful = {\n",
    "            \"feedback_type\": \"solution_not_helpful\",\n",
    "            \"solution_ticket_id\": pipeline_results['recommended_solutions'][1]['ticket_id'],\n",
    "            \"reason\": \"Issue was different\",\n",
    "            \"agent_id\": \"agent007\"\n",
    "        }\n",
    "        log_feedback_notebook(sample_ticket_id, \"retriever\", feedback_data_not_helpful)\n",
    "\n",
    "# Simulate feedback on the predicted queue\n",
    "feedback_data_queue = {\n",
    "    \"feedback_type\": \"correct_queue\", # or \"incorrect_queue\"\n",
    "    \"predicted_queue\": pipeline_results.get('predicted_queue'),\n",
    "    # \"correct_queue\": \"Networking\", # Provide if incorrect\n",
    "    \"agent_id\": \"agent007\"\n",
    "}\n",
    "log_feedback_notebook(sample_ticket_id, \"router\", feedback_data_queue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e85ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
